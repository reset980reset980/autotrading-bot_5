# ğŸ“ íŒŒì¼ëª…: modules/attention_model.py
# ğŸ¯ ëª©ì : Attention ê¸°ë°˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ë¡œ ì‹œê³„ì—´ ê°€ê²© íë¦„ ë° ê°ì • ì ìˆ˜ ì˜ˆì¸¡
# ğŸ” ì „ì²´ íë¦„ë„:
#     - í•™ìŠµ: ë‰´ìŠ¤ ê°ì • ì ìˆ˜ + ê¸°ìˆ  ì§€í‘œ + ì‹œì„¸ â†’ ë¯¸ë˜ ì‹œê·¸ë„ ì˜ˆì¸¡
#     - ì˜ˆì¸¡: í˜„ì¬ ì§€í‘œ ì…ë ¥ â†’ LONG / SHORT / HOLD ì‹œê·¸ë„ ì¶œë ¥
# ğŸ”§ ì£¼ìš” í•¨ìˆ˜:
#     - train_model(): ê³¼ê±° ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ
#     - predict_signal(): í˜„ì¬ ì§€í‘œ ê¸°ë°˜ ì‹œê·¸ë„ ì˜ˆì¸¡
#     - save_model() / load_model(): ëª¨ë¸ ì €ì¥ ë° ë¶ˆëŸ¬ì˜¤ê¸°
# ğŸ’¬ í”„ë¡¬í”„íŠ¸ ìš”ì•½:
#     â–¶ "ê¸°ìˆ  ì§€í‘œì™€ ê°ì • ë°ì´í„°ë¥¼ ì…ë ¥ ë°›ì•„, ë”¥ëŸ¬ë‹ìœ¼ë¡œ ì „ëµ ì‹œê·¸ë„ì„ ì˜ˆì¸¡í•˜ë¼."

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import os

MODEL_PATH = "models/attention_model.pt"

# ğŸ”¹ Attention ê¸°ë°˜ ê°„ë‹¨í•œ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ ì •ì˜
class AttentionModel(nn.Module):
    def __init__(self, input_size, hidden_size=64):
        super(AttentionModel, self).__init__()
        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)
        self.attn = nn.Linear(hidden_size, 1)
        self.out = nn.Linear(hidden_size, 3)  # [LONG, SHORT, HOLD]

    def forward(self, x):
        out, _ = self.rnn(x)
        weights = torch.softmax(self.attn(out), dim=1)
        context = torch.sum(weights * out, dim=1)
        return self.out(context)

# ğŸ”¸ ì‹œê·¸ë„ ì¸ì½”ë”©
def encode_signal(signal: str) -> int:
    return {"long": 0, "short": 1, "hold": 2}.get(signal, 2)

def decode_signal(index: int) -> str:
    return ["long", "short", "hold"][index]

# ğŸ”¹ ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜
def train_model(df: pd.DataFrame):
    model = AttentionModel(input_size=6)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # ì…ë ¥: [rsi, macd, ema, tema, sentiment, price], íƒ€ê²Ÿ: signal
    X = df[["rsi", "macd", "ema", "tema", "sentiment", "close"]].values
    y = df["signal"].apply(encode_signal).values

    X_tensor = torch.tensor(X, dtype=torch.float32).unsqueeze(1)
    y_tensor = torch.tensor(y, dtype=torch.long)

    for epoch in range(50):
        optimizer.zero_grad()
        output = model(X_tensor)
        loss = criterion(output, y_tensor)
        loss.backward()
        optimizer.step()

    os.makedirs("models", exist_ok=True)
    torch.save(model.state_dict(), MODEL_PATH)

# ğŸ”¹ ì˜ˆì¸¡ í•¨ìˆ˜
def predict_signal(indicators: dict, sentiment_score: float):
    model = AttentionModel(input_size=6)
    if not os.path.exists(MODEL_PATH):
        return "hold"

    model.load_state_dict(torch.load(MODEL_PATH))
    model.eval()

    x = np.array([[indicators["rsi"], indicators["macd"], indicators["ema"],
                   indicators["tema"], sentiment_score, indicators["close"]]])
    x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(1)
    with torch.no_grad():
        output = model(x_tensor)
        prediction = torch.argmax(output, dim=1).item()
    return decode_signal(prediction)
