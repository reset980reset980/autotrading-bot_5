# ğŸ“ íŒŒì¼ëª…: modules/model_evaluator.py
# ğŸ¯ ëª©ì : ì „ëµ íŒë‹¨ ê²°ê³¼ì— ëŒ€í•œ ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨, F1, AUC ë“± ì„±ëŠ¥ ì§€í‘œ ì‚°ì¶œ
# ğŸ” ì „ì²´ íë¦„ë„:
#     - ì˜ˆì¸¡ ê²°ê³¼ vs ì‹¤ì œ ê²°ê³¼ ë¹„êµ
#     - ë¶„ë¥˜ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
#     - í™•ë¥  ì˜ˆì¸¡(probs) ê¸°ë°˜ AUC ì œê³µ (ì„ íƒ)
# ğŸ”§ ì£¼ìš” í•¨ìˆ˜:
#     - evaluate_strategy_performance(): ì„±ëŠ¥ ì§€í‘œ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
# ğŸ’¬ ì‘ì—… í”„ë¡¬í”„íŠ¸ ìš”ì•½:
#     â–¶ "AI ì „ëµ íŒë‹¨ì˜ ì„±ëŠ¥ì„ ìˆ˜ì¹˜ë¡œ í‰ê°€í•˜ë¼."

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from typing import List, Optional, Dict

def evaluate_strategy_performance(predictions: List[int], actuals: List[int], probs: Optional[List[float]] = None) -> Dict[str, float]:
    """
    ì „ëµ íŒë‹¨ ê²°ê³¼ì— ëŒ€í•œ ë¶„ë¥˜ ì„±ëŠ¥ ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

    Args:
        predictions (List[int]): ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ (0 or 1)
        actuals (List[int]): ì‹¤ì œ í´ë˜ìŠ¤ ë ˆì´ë¸”
        probs (Optional[List[float]]): ì–‘ì„± í´ë˜ìŠ¤ì˜ í™•ë¥  ì˜ˆì¸¡ ê°’ (AUC ê³„ì‚°ì— í•„ìš”)

    Returns:
        Dict[str, float]: accuracy, precision, recall, f1_score, auc í¬í•¨í•œ í‰ê°€ ê²°ê³¼
    """
    results = {
        "accuracy": accuracy_score(actuals, predictions),
        "precision": precision_score(actuals, predictions, zero_division=0),
        "recall": recall_score(actuals, predictions, zero_division=0),
        "f1_score": f1_score(actuals, predictions, zero_division=0),
    }

    try:
        results["auc"] = roc_auc_score(actuals, probs) if probs else None
    except Exception:
        results["auc"] = None

    return results


# âœ… ì˜ˆì‹œ ì‚¬ìš©
if __name__ == "__main__":
    preds = [1, 0, 1, 1, 0]
    actuals = [1, 0, 0, 1, 0]
    probs = [0.92, 0.15, 0.73, 0.87, 0.22]

    result = evaluate_strategy_performance(preds, actuals, probs)
    print("ğŸ“Š ì „ëµ í‰ê°€ ê²°ê³¼:", result)
